1
00:00:00,000 --> 00:00:06,000
Hello guys, so thank you for considering this amazing course, which is a complete comprehensive MLOps

2
00:00:06,000 --> 00:00:08,000
course with end to end data science project.

3
00:00:09,000 --> 00:00:14,000
In this video, I will be talking about the life cycle of a data science project.

4
00:00:14,000 --> 00:00:17,000
What does a big data engineer specifically do?

5
00:00:17,000 --> 00:00:19,000
What does a data scientist actually do?

6
00:00:20,000 --> 00:00:20,000
You know?

7
00:00:20,000 --> 00:00:27,000
And in this entire course, what all MLOps tools we are going to specifically cover and how it is basically

8
00:00:27,000 --> 00:00:30,000
going to use in which module it is basically going to use.

9
00:00:30,000 --> 00:00:31,000
We'll talk about them.

10
00:00:31,000 --> 00:00:32,000
Right.

11
00:00:32,000 --> 00:00:37,000
So if you probably see this, I've created so many diagrams and here I have, I have just put up the

12
00:00:37,000 --> 00:00:41,000
logo of every MLOps tools that we are covering in this course.

13
00:00:41,000 --> 00:00:41,000
Okay.

14
00:00:41,000 --> 00:00:48,000
So we have DVC, we have MLflow, we have Apache Airflow, we have astronomer, we have git, we have

15
00:00:48,000 --> 00:00:53,000
GitHub, we have AWS, we have Docker, we have GitHub action.

16
00:00:53,000 --> 00:00:55,000
We have Amazon SageMaker.

17
00:00:55,000 --> 00:00:57,000
We have Grafana.

18
00:00:57,000 --> 00:00:58,000
We have MongoDB.

19
00:00:58,000 --> 00:00:59,000
We have Postgres Grace.

20
00:00:59,000 --> 00:01:01,000
And we have Python.

21
00:01:01,000 --> 00:01:06,000
Um, along with this, you know, one more cloud that I've actually used in one of the project is about

22
00:01:06,000 --> 00:01:07,000
Azure.

23
00:01:07,000 --> 00:01:07,000
Okay.

24
00:01:07,000 --> 00:01:09,000
So Azure is also covered over here.

25
00:01:09,000 --> 00:01:14,000
Uh, I've shown the deployment how you can probably convert that into a Docker container and probably

26
00:01:14,000 --> 00:01:16,000
deploy in Azure Web container itself.

27
00:01:16,000 --> 00:01:16,000
Right.

28
00:01:16,000 --> 00:01:17,000
Web app itself.

29
00:01:17,000 --> 00:01:20,000
So everything will be discussing in this specific course.

30
00:01:20,000 --> 00:01:21,000
But before going ahead.

31
00:01:21,000 --> 00:01:21,000
Right.

32
00:01:21,000 --> 00:01:27,000
You really need to understand what is an MLOps engineer specifically do in an end to end data science

33
00:01:27,000 --> 00:01:28,000
project.

34
00:01:28,000 --> 00:01:31,000
So first I will talk about the life cycle of a data science project.

35
00:01:31,000 --> 00:01:37,000
So if you probably see this two diagrams, this one diagram and the other diagram over here, this is

36
00:01:37,000 --> 00:01:40,000
more about the life cycle of a data science project.

37
00:01:40,000 --> 00:01:45,000
And then what we'll do is that while we are discussing this life cycle, where all which all MLOps tools

38
00:01:45,000 --> 00:01:50,000
will be using, where, you know, because in our end to end data science project that we have discussed

39
00:01:50,000 --> 00:01:53,000
in this particular course, we will also be including this entire MLOps tool.

40
00:01:53,000 --> 00:01:54,000
Okay.

41
00:01:54,000 --> 00:02:00,000
So here, um, see, understand in order to understand the life cycle of a data science project.

42
00:02:00,000 --> 00:02:05,000
So first of all, let's say that there is a data science project or a big data engineering project,

43
00:02:05,000 --> 00:02:05,000
right?

44
00:02:05,000 --> 00:02:12,000
So first of all, the first task, the first step is basically with the requirement that gatherings,

45
00:02:12,000 --> 00:02:18,000
you know, now in the requirement gathering, we have uh, domain experts or product owner along with

46
00:02:18,000 --> 00:02:19,000
business analyst.

47
00:02:19,000 --> 00:02:24,000
So if you are probably seeing in a data science team, uh, they will only not be data scientist or

48
00:02:24,000 --> 00:02:25,000
data analyst.

49
00:02:25,000 --> 00:02:30,000
They will be a lot of different roles and responsibilities like domain expert or product owner.

50
00:02:30,000 --> 00:02:32,000
You'll also be having business analyst.

51
00:02:32,000 --> 00:02:33,000
And all right.

52
00:02:33,000 --> 00:02:36,000
So let's say that these two group of people are like these two people.

53
00:02:36,000 --> 00:02:36,000
Right.

54
00:02:36,000 --> 00:02:41,000
And when I say domain expert or product owner, this person will be having an extreme good amount of

55
00:02:41,000 --> 00:02:45,000
domain knowledge with respect to this particular data science project.

56
00:02:45,000 --> 00:02:48,000
Let's say this particular project is coming from a product based company.

57
00:02:49,000 --> 00:02:49,000
Right.

58
00:02:49,000 --> 00:02:53,000
So the data science project may be coming from a retail department.

59
00:02:53,000 --> 00:02:56,000
It may be coming from the e-commerce department.

60
00:02:56,000 --> 00:02:58,000
It may be coming from different different departments.

61
00:02:58,000 --> 00:02:58,000
Right.

62
00:02:58,000 --> 00:03:03,000
And here with respect to this particular use case, we are going to solve this data science project.

63
00:03:03,000 --> 00:03:07,000
So obviously the person who is experienced in this domain will be the product owner.

64
00:03:07,000 --> 00:03:08,000
Right.

65
00:03:08,000 --> 00:03:13,000
So this person will know that what is that data science project that they really want to implement?

66
00:03:13,000 --> 00:03:13,000
Okay.

67
00:03:13,000 --> 00:03:16,000
Now this person along with the business analyst.

68
00:03:16,000 --> 00:03:18,000
So here business analyst are the most luckiest people.

69
00:03:18,000 --> 00:03:27,000
What I feel because if your if your uh uh if your specifically like if your client is in some different

70
00:03:27,000 --> 00:03:32,000
foreign countries and also you really get a good amount of chance to interact with them travel, travel

71
00:03:32,000 --> 00:03:33,000
the whole world itself.

72
00:03:33,000 --> 00:03:34,000
Right.

73
00:03:34,000 --> 00:03:40,000
So this business analyst, along with the domain expert or product owner, they will have a lot of discussion

74
00:03:40,000 --> 00:03:42,000
and they will jot down all the requirements.

75
00:03:42,000 --> 00:03:42,000
Right.

76
00:03:42,000 --> 00:03:45,000
So uh, they will write down all the requirements.

77
00:03:45,000 --> 00:03:48,000
So here the entire requirements will be prepared.

78
00:03:48,000 --> 00:03:55,000
And they will write down all the requirements like uh, what all stories needs to be completed and they

79
00:03:55,000 --> 00:03:59,000
will try to divide this entire story in the form of sprints.

80
00:03:59,000 --> 00:04:04,000
Okay, so here in a data science project, also we are in most of the companies.

81
00:04:04,000 --> 00:04:06,000
They follow an agile process.

82
00:04:06,000 --> 00:04:11,000
Now agile process basically means like they are different different process of development.

83
00:04:11,000 --> 00:04:13,000
You know we specifically use agile process.

84
00:04:13,000 --> 00:04:15,000
There is like waterfall model and all.

85
00:04:15,000 --> 00:04:19,000
So in most of the data science project, what I have seen in my experience, they usually follow an

86
00:04:19,000 --> 00:04:24,000
agile process where they create sprints like there will be sprint one, there will be sprint two, and

87
00:04:24,000 --> 00:04:30,000
each and every sprint they will try to divide this requirements like which all modules needs to be developed

88
00:04:30,000 --> 00:04:33,000
in sprint one, which all module needs to be developed in sprint two.

89
00:04:33,000 --> 00:04:39,000
So once that requirements is basically written down okay, then all these requirements are sent to the

90
00:04:39,000 --> 00:04:43,000
data analyst or data scientist team specifically in a data science project.

91
00:04:43,000 --> 00:04:47,000
Now again, this data analyst and data science team will have a discussion with the domain expertise

92
00:04:47,000 --> 00:04:55,000
or product owner and they will try to find out like what all data, what all data will be required in

93
00:04:55,000 --> 00:04:57,000
order to solve this particular project.

94
00:04:57,000 --> 00:04:57,000
Okay.

95
00:04:57,000 --> 00:04:58,000
Solve this particular problem.

96
00:04:58,000 --> 00:05:02,000
So they will be identifying the data right.

97
00:05:02,000 --> 00:05:05,000
They will be specifically identifying the data.

98
00:05:05,000 --> 00:05:11,000
Now when I say identifying the data that basically means, uh, you know, they'll try to identify the

99
00:05:11,000 --> 00:05:14,000
source of the data okay.

100
00:05:14,000 --> 00:05:17,000
Now data may be present in the internal database.

101
00:05:17,000 --> 00:05:20,000
It may be present in a third party or cloud API.

102
00:05:20,000 --> 00:05:21,000
It may also be an API data.

103
00:05:21,000 --> 00:05:23,000
It may be an IoT data also.

104
00:05:23,000 --> 00:05:24,000
Right.

105
00:05:24,000 --> 00:05:29,000
Uh, in in my previous companies where I was working with electronics goods, you know, there we had

106
00:05:29,000 --> 00:05:32,000
lot of IoT devices and we used to capture those specific data.

107
00:05:32,000 --> 00:05:36,000
So we need to identify the data and to identify the data.

108
00:05:36,000 --> 00:05:40,000
Again, we will be requiring the help of a product owner or domain expertise because they may also be

109
00:05:40,000 --> 00:05:42,000
having the data within the company.

110
00:05:42,000 --> 00:05:45,000
They may be dependent on some third parties and all.

111
00:05:45,000 --> 00:05:48,000
So those kind of data, they will first of all identify.

112
00:05:48,000 --> 00:05:55,000
Once they identify the data source then here my big data engineering team will come into picture now,

113
00:05:55,000 --> 00:06:00,000
because this big data engineering team will now create a data pipeline.

114
00:06:02,000 --> 00:06:03,000
Data pipeline.

115
00:06:03,000 --> 00:06:09,000
Now, this data pipeline is created in such a way that, first of all, whichever source we are identifying,

116
00:06:09,000 --> 00:06:14,000
right, we will capture all this particular source, and then we will combine all those data in the

117
00:06:14,000 --> 00:06:15,000
form of data pipeline.

118
00:06:15,000 --> 00:06:16,000
Right.

119
00:06:16,000 --> 00:06:20,000
Now, one of the most common data pipeline that you will be seeing in the industries that will be done

120
00:06:20,000 --> 00:06:23,000
by the big data engineering team, it is nothing but the ETL pipeline.

121
00:06:24,000 --> 00:06:26,000
Okay, ETL pipeline.

122
00:06:26,000 --> 00:06:30,000
Now this ETL pipeline is nothing but extract, transform and load okay.

123
00:06:30,000 --> 00:06:33,000
We basically extract from multiple data sources.

124
00:06:33,000 --> 00:06:38,000
We do some kind of transformation, and then we finally load that entire combined data into some data

125
00:06:38,000 --> 00:06:39,000
source like MongoDB.

126
00:06:39,000 --> 00:06:41,000
It can be PostgreSQL and all.

127
00:06:41,000 --> 00:06:44,000
So here we will be covering this entire ETL pipeline.

128
00:06:44,000 --> 00:06:49,000
Also I have taken a couple of projects and I will show you how an ETL pipeline will be used.

129
00:06:49,000 --> 00:06:52,000
And we will specifically use Apache Airflow over here.

130
00:06:52,000 --> 00:06:58,000
And this is one of the amazing MLOps tool, uh, which is basically used to create this ETL pipeline,

131
00:06:58,000 --> 00:07:02,000
because this actually helps you to schedule the right schedule.

132
00:07:02,000 --> 00:07:06,000
Because if let's say that my data is basically coming from third party API every day, I need to probably

133
00:07:06,000 --> 00:07:10,000
get that particular data so I can actually schedule this with the help of Apache Airflow.

134
00:07:10,000 --> 00:07:10,000
Okay.

135
00:07:10,000 --> 00:07:12,000
And we will be creating this ETL pipeline.

136
00:07:12,000 --> 00:07:18,000
So here in this MLOps project, you know, my main aim was to give you the, uh, the crux of each and

137
00:07:18,000 --> 00:07:18,000
everything.

138
00:07:18,000 --> 00:07:20,000
Like how does a big data engineering team work?

139
00:07:20,000 --> 00:07:25,000
Because here we'll focus on deployment also how to deploy this entire ETL pipeline.

140
00:07:25,000 --> 00:07:27,000
So I will be focusing on that right.

141
00:07:27,000 --> 00:07:31,000
So in this particular uh like in one of the modules right.

142
00:07:31,000 --> 00:07:33,000
We will be discussing about data pipeline.

143
00:07:33,000 --> 00:07:35,000
We will be discussing about ETL pipeline.

144
00:07:35,000 --> 00:07:37,000
We'll also be doing the deployment.

145
00:07:38,000 --> 00:07:40,000
And for this we'll be using AWS cloud and all.

146
00:07:40,000 --> 00:07:40,000
Okay.

147
00:07:40,000 --> 00:07:44,000
So uh that is what a big data engineer team does.

148
00:07:44,000 --> 00:07:49,000
They actually create amazing data pipelines wherein they integrate with multiple data sources and then

149
00:07:49,000 --> 00:07:53,000
try to combine the data and they load it into a specific data source.

150
00:07:53,000 --> 00:07:57,000
Once that data source is, let's say the data source over here is used as MongoDB, right?

151
00:07:57,000 --> 00:08:02,000
So once this entire data is basically loaded in the MongoDB, this will keep on getting updated in a

152
00:08:02,000 --> 00:08:03,000
daily basis or in a weekly basis.

153
00:08:03,000 --> 00:08:05,000
That depends on the data.

154
00:08:05,000 --> 00:08:08,000
That is probably coming from the third party APIs or any other data source.

155
00:08:08,000 --> 00:08:09,000
Okay.

156
00:08:09,000 --> 00:08:13,000
Now once this is done, then our life cycle of a data science project comes.

157
00:08:13,000 --> 00:08:18,000
Now, just by understanding this, you are able to understand that how many different roles and responsibilities

158
00:08:18,000 --> 00:08:19,000
are there, right?

159
00:08:19,000 --> 00:08:22,000
One of the roles and responsibilities over here are like product owner.

160
00:08:22,000 --> 00:08:26,000
Then you have business analyst, then you have data scientist, then data analyst, then you have big

161
00:08:26,000 --> 00:08:27,000
data engineering team, right.

162
00:08:27,000 --> 00:08:32,000
And this entire MLOps course I will cover each and every thing with respect to this, right.

163
00:08:32,000 --> 00:08:37,000
Obviously, I will not be covering product owner or business analyst because that is more of a documentation

164
00:08:37,000 --> 00:08:38,000
kind of things.

165
00:08:38,000 --> 00:08:42,000
They use tools like JIRA confluence to write down all the requirements.

166
00:08:42,000 --> 00:08:47,000
But here from this, identifying the data source and all, we will try to go ahead and implement it.

167
00:08:47,000 --> 00:08:49,000
Now, once we go to the life cycle of a data science project.

168
00:08:49,000 --> 00:08:53,000
Now the next thing will be that whatever data has been collected in the data source, right?

169
00:08:54,000 --> 00:08:59,000
So let's say over here I will just try to connect this okay.

170
00:08:59,000 --> 00:09:01,000
I will just try to go ahead and connect this.

171
00:09:01,000 --> 00:09:03,000
Let's say this is my data source.

172
00:09:03,000 --> 00:09:08,000
Uh, one of the data source that I have also used is like something called as PostgreSQL.

173
00:09:08,000 --> 00:09:08,000
Okay.

174
00:09:08,000 --> 00:09:10,000
It can be Postgres, it can be MongoDB.

175
00:09:10,000 --> 00:09:13,000
In one of the example I have used MongoDB.

176
00:09:13,000 --> 00:09:13,000
Okay.

177
00:09:13,000 --> 00:09:20,000
So let's consider MongoDB MongoDB and this MongoDB will also be hosted in the cloud okay.

178
00:09:20,000 --> 00:09:24,000
Now from this particular uh pipeline we are storing it in the MongoDB.

179
00:09:25,000 --> 00:09:29,000
Now from the MongoDB, our life cycle of a data science project starts okay.

180
00:09:30,000 --> 00:09:32,000
Now again, here we are.

181
00:09:32,000 --> 00:09:37,000
Whenever we talk about life cycle or data science project, the first step, which I probably missed

182
00:09:37,000 --> 00:09:41,000
over here, to write it down, it is nothing but data ingestion.

183
00:09:42,000 --> 00:09:49,000
So here it is nothing but data Ingestion.

184
00:09:51,000 --> 00:09:53,000
So this is the step before feature engineering.

185
00:09:53,000 --> 00:09:58,000
So I would like to consider as this as my first step okay.

186
00:09:58,000 --> 00:10:01,000
Then I go to this my next step.

187
00:10:01,000 --> 00:10:03,000
Or let me do one thing.

188
00:10:03,000 --> 00:10:08,000
Let me just draw it on the top so that it will be easy for you to understand.

189
00:10:08,000 --> 00:10:13,000
Okay, so here my first step is nothing but data ingestion.

190
00:10:14,000 --> 00:10:18,000
Data ingestion okay.

191
00:10:19,000 --> 00:10:20,000
So here is my first step.

192
00:10:20,000 --> 00:10:23,000
Then we go to feature engineering feature selection and all.

193
00:10:23,000 --> 00:10:25,000
Now what is the work in data ingestion here.

194
00:10:25,000 --> 00:10:32,000
Uh our main aim is to read the data from data source okay.

195
00:10:33,000 --> 00:10:39,000
So whenever I talk about data ingestion I will be showing you how to read the data from cloud.

196
00:10:39,000 --> 00:10:43,000
Let's say how to read the data from S3 bucket or AWS S3 bucket.

197
00:10:43,000 --> 00:10:49,000
How to read the data from MongoDB It can also be how to read data from Postgres.

198
00:10:49,000 --> 00:10:50,000
Right.

199
00:10:50,000 --> 00:10:53,000
So all these kind of examples, we will try to cover it up.

200
00:10:53,000 --> 00:10:53,000
Okay.

201
00:10:53,000 --> 00:10:55,000
In the data ingestion.

202
00:10:55,000 --> 00:10:56,000
Right.

203
00:10:56,000 --> 00:11:03,000
And uh while we are probably, uh reading it, there will be separate MLOps tools that will be required.

204
00:11:03,000 --> 00:11:11,000
One of the important MLOps tools that will be used over here is something called as DVC, DVC.

205
00:11:11,000 --> 00:11:13,000
DVC basically means data versioning control.

206
00:11:13,000 --> 00:11:14,000
Okay.

207
00:11:14,000 --> 00:11:17,000
So here you will be able to see that I have used DVC.

208
00:11:17,000 --> 00:11:21,000
So we'll be talking about like why DVC will be important over here.

209
00:11:21,000 --> 00:11:25,000
Because data versioning is also important because with respect to every training that we do for our

210
00:11:25,000 --> 00:11:30,000
model, right, it is important that we have a proper versioning of the data because tomorrow the data

211
00:11:30,000 --> 00:11:30,000
may change.

212
00:11:30,000 --> 00:11:33,000
It may also add some more features inside it.

213
00:11:33,000 --> 00:11:37,000
It may change the entire data format itself which is coming from the data source.

214
00:11:37,000 --> 00:11:37,000
Okay.

215
00:11:37,000 --> 00:11:43,000
So here, uh, if I talk about DVC, DVC is one MLOps tools, which I'm actually going to use over here

216
00:11:43,000 --> 00:11:44,000
in big data engineering team.

217
00:11:44,000 --> 00:11:49,000
While we are creating the data pipeline, we will be specifically using airflow, you know, to probably

218
00:11:49,000 --> 00:11:51,000
create the entire ETL pipeline.

219
00:11:51,000 --> 00:11:53,000
Then after this, we go to the feature engineering.

220
00:11:53,000 --> 00:11:58,000
Again, in this course we have not focused more on feature engineering or different types of feature

221
00:11:58,000 --> 00:11:58,000
engineering.

222
00:11:58,000 --> 00:12:01,000
Otherwise this course will go on very long.

223
00:12:01,000 --> 00:12:06,000
But some basic feature engineering so that I show you in a specific end to end project complete end

224
00:12:06,000 --> 00:12:07,000
to end project.

225
00:12:07,000 --> 00:12:12,000
But, uh, if you probably see some other courses of mine like machine learning and all that, I have

226
00:12:12,000 --> 00:12:15,000
focused more on feature engineering and feature selection.

227
00:12:15,000 --> 00:12:15,000
Okay.

228
00:12:15,000 --> 00:12:20,000
So this is one of the module like feature transformation where we specifically do EDA handling missing

229
00:12:20,000 --> 00:12:21,000
values, handling outliers and all.

230
00:12:21,000 --> 00:12:26,000
Then we have feature selection where we probably select the best features that is required for the model.

231
00:12:26,000 --> 00:12:28,000
Now this is the most important step.

232
00:12:28,000 --> 00:12:30,000
That is model creation and hyperparameter tuning.

233
00:12:30,000 --> 00:12:34,000
Now for model creation hyperparameter tuning I will be using multiple models.

234
00:12:34,000 --> 00:12:39,000
We will try to perform different different hyperparameter tuning will try to select the different parameters.

235
00:12:39,000 --> 00:12:41,000
One example I've shown with the help of Hyperopt.

236
00:12:42,000 --> 00:12:44,000
One example I've shown with Gridsearchcv.

237
00:12:44,000 --> 00:12:44,000
CV.

238
00:12:44,000 --> 00:12:46,000
And then you can also go ahead and try out.

239
00:12:46,000 --> 00:12:48,000
I've also used Randomizedsearchcv in one example.

240
00:12:48,000 --> 00:12:49,000
Okay.

241
00:12:49,000 --> 00:12:53,000
So you will be able to see all this kind of hyperparameter tuning when I am doing it.

242
00:12:53,000 --> 00:12:58,000
And the reason why I'm doing this, because after I perform the hyperparameter tuning, I have to log

243
00:12:58,000 --> 00:12:59,000
each and every metrics.

244
00:13:00,000 --> 00:13:08,000
And here I will be using something called as MLflow, because here we are going to probably track experiments.

245
00:13:08,000 --> 00:13:11,000
So experiment tracking will happen over here.

246
00:13:12,000 --> 00:13:14,000
Experiment tracking will happen over here.

247
00:13:15,000 --> 00:13:17,000
Now this experiment tracking how it is basically happening.

248
00:13:17,000 --> 00:13:21,000
And here we are specifically going to use MLflow.

249
00:13:21,000 --> 00:13:25,000
Not only that will deploy this MLflow in remote repository or in AWS.

250
00:13:25,000 --> 00:13:28,000
We are going to use a remote repository called a DAGs hub.

251
00:13:28,000 --> 00:13:29,000
Right.

252
00:13:29,000 --> 00:13:32,000
So if I talk about DAGs hub this is another remote repository.

253
00:13:33,000 --> 00:13:34,000
We will be using that also.

254
00:13:34,000 --> 00:13:35,000
Right.

255
00:13:35,000 --> 00:13:38,000
So here specifically we will be using MLflow for experiment tracking.

256
00:13:38,000 --> 00:13:43,000
And here you can see that MLflow is also one of the MLOps tools that we will be using over here.

257
00:13:43,000 --> 00:13:47,000
my complete experiment tracking where we will be logging metrics, where we'll be logging each and every

258
00:13:47,000 --> 00:13:48,000
parameter.

259
00:13:48,000 --> 00:13:48,000
Okay.

260
00:13:49,000 --> 00:13:54,000
Then, uh, we will be understanding about dockers, how to dockerize your container and all.

261
00:13:54,000 --> 00:13:59,000
So here, uh, there is a detailed module about understanding about Dockers, how you can work with

262
00:13:59,000 --> 00:14:00,000
Dockers and all.

263
00:14:00,000 --> 00:14:04,000
Uh, everything like Docker compose, everything is basically explained in that particular module.

264
00:14:04,000 --> 00:14:08,000
Then we have this model deployment next stage.

265
00:14:08,000 --> 00:14:13,000
Uh, when I talk about model deployment, um, I'll be using clouds like AWS and Azure.

266
00:14:13,000 --> 00:14:13,000
Okay.

267
00:14:13,000 --> 00:14:16,000
So two clouds will be more than sufficient in this particular course.

268
00:14:16,000 --> 00:14:21,000
In one of the course, I have, uh, in one of the module I have implemented both with AWS Azure.

269
00:14:21,000 --> 00:14:23,000
In other I have specifically gone with AWS.

270
00:14:23,000 --> 00:14:27,000
You know, um, I have also used, uh, AWS SageMaker.

271
00:14:27,000 --> 00:14:32,000
So AWS SageMaker actually helps you to create the entire entire end to end, uh, you'll be able to

272
00:14:33,000 --> 00:14:37,000
build, deploy, create endpoints directly inside your AWS infrastructure itself.

273
00:14:37,000 --> 00:14:40,000
So that project is also probably taken over here.

274
00:14:40,000 --> 00:14:41,000
Okay.

275
00:14:41,000 --> 00:14:44,000
Then coming to the last stage, which is nothing but model monitoring.

276
00:14:44,000 --> 00:14:44,000
Okay.

277
00:14:44,000 --> 00:14:45,000
Now model monitoring.

278
00:14:45,000 --> 00:14:52,000
Uh, over here will we will try to visualize the, uh, model outputs or we'll try to write some rules

279
00:14:52,000 --> 00:14:53,000
based on that.

280
00:14:53,000 --> 00:14:54,000
We'll probably visualize things.

281
00:14:54,000 --> 00:14:59,000
And for this also we are going to use an MLOps tool which is called as Grafana.

282
00:14:59,000 --> 00:14:59,000
Okay.

283
00:14:59,000 --> 00:15:02,000
So I will be using something called as Grafana.

284
00:15:02,000 --> 00:15:04,000
And if I just show you the logo over here.

285
00:15:04,000 --> 00:15:06,000
So Grafana is basically used.

286
00:15:06,000 --> 00:15:06,000
Okay.

287
00:15:07,000 --> 00:15:12,000
Uh, so this is the overall thing because based on this particular visualization that we are created

288
00:15:12,000 --> 00:15:13,000
in the Grafana.

289
00:15:13,000 --> 00:15:17,000
And I'll try to show you through the basic example like how a rules is basically set in the Grafana,

290
00:15:17,000 --> 00:15:22,000
how the visualization is basically created, how each and every graph is created, and all that we'll

291
00:15:22,000 --> 00:15:23,000
be discussing over here.

292
00:15:23,000 --> 00:15:23,000
Okay.

293
00:15:23,000 --> 00:15:31,000
So overall, uh, in this entire, uh, entire lifecycle of a data science project, creating life cycle

294
00:15:31,000 --> 00:15:33,000
or creating a data science project will be very easy.

295
00:15:33,000 --> 00:15:38,000
Uh, like you can also use a Jupyter notebook and probably create each and every thing.

296
00:15:38,000 --> 00:15:41,000
But my main aim is to bring modularity.

297
00:15:42,000 --> 00:15:45,000
Modularity in the.

298
00:15:45,000 --> 00:15:50,000
We will try to write a modular code how an end to end project is basically created completely modular

299
00:15:50,000 --> 00:15:52,000
how industries in industry.

300
00:15:52,000 --> 00:15:56,000
We specifically create we will be using YAML, YAML files.

301
00:15:56,000 --> 00:15:56,000
Right?

302
00:15:56,000 --> 00:15:59,000
How to read a YAML file like params, YAML schema, YAML files.

303
00:15:59,000 --> 00:16:05,000
Along with this are how how a complete pipeline is basically created.

304
00:16:05,000 --> 00:16:09,000
We will be discussing about that and whenever we do the deployment right we do the deployment.

305
00:16:09,000 --> 00:16:13,000
Over here we will be using something called as GitHub action.

306
00:16:13,000 --> 00:16:14,000
Okay.

307
00:16:14,000 --> 00:16:19,000
Now this GitHub action will actually help us to do the automated deployment okay.

308
00:16:19,000 --> 00:16:20,000
Automated deployment.

309
00:16:20,000 --> 00:16:24,000
So here we will be also showing you that how the GitHub actions can be created.

310
00:16:24,000 --> 00:16:26,000
You know what are workflows.

311
00:16:26,000 --> 00:16:32,000
How do you probably go ahead and create different different environments like you at or this that you

312
00:16:32,000 --> 00:16:33,000
know it is up to you, right.

313
00:16:33,000 --> 00:16:38,000
So that is what my main aim is to make it completely industry standard.

314
00:16:38,000 --> 00:16:45,000
And the best thing about this course is that everything over here, all the tools are open source,

315
00:16:46,000 --> 00:16:48,000
all the tools are open source.

316
00:16:48,000 --> 00:16:48,000
Okay.

317
00:16:49,000 --> 00:16:55,000
Now many people will be asking, now let me just go through the list of all the MLOps tools we are using.

318
00:16:55,000 --> 00:16:55,000
So DVC.

319
00:16:55,000 --> 00:17:00,000
I've already told you, I'll make you understand that in the data ingestion, we'll be using MLflow

320
00:17:00,000 --> 00:17:05,000
specifically in the experiment tracking airflow in the creation of the ETL pipeline.

321
00:17:05,000 --> 00:17:07,000
Then now there is something called as astronomer.

322
00:17:07,000 --> 00:17:13,000
So astronomer actually provides a framework to manage the airflow okay.

323
00:17:13,000 --> 00:17:16,000
So this specifically manages the airflow.

324
00:17:17,000 --> 00:17:21,000
And along with this it provides a cloud platform to properly deploy the airflow.

325
00:17:21,000 --> 00:17:29,000
So we'll be talking about astronomer then for my code tracking I will be using git right.

326
00:17:29,000 --> 00:17:32,000
For the code tracking code commit I will be using git.

327
00:17:32,000 --> 00:17:40,000
GitHub will be my repository because it is completely available for free Then AWS is the cloud platform

328
00:17:40,000 --> 00:17:40,000
we'll be using.

329
00:17:40,000 --> 00:17:42,000
We'll also be using AWS SageMaker.

330
00:17:42,000 --> 00:17:48,000
As I said, to define workflows and all, we will be creating GitHub actions to Dockerize.

331
00:17:48,000 --> 00:17:51,000
Our entire application will be using Docker's right.

332
00:17:51,000 --> 00:17:53,000
Then you have Grafana.

333
00:17:53,000 --> 00:17:57,000
This is specifically for visualization so that we'll be able to check the performance of our model.

334
00:17:57,000 --> 00:18:00,000
Databases we will be specifically using MongoDB.

335
00:18:00,000 --> 00:18:04,000
So again understand that I'm not going to teach completely from basics.

336
00:18:04,000 --> 00:18:10,000
Uh, I'm considering this that, you know, at least one NoSQL or one SQL database.

337
00:18:10,000 --> 00:18:10,000
Right.

338
00:18:10,000 --> 00:18:13,000
Because I'll directly be writing the query and all.

339
00:18:13,000 --> 00:18:18,000
And finally, the Python programming language, uh, is the programming language that we are going to

340
00:18:18,000 --> 00:18:18,000
use.

341
00:18:18,000 --> 00:18:19,000
Okay.

342
00:18:19,000 --> 00:18:25,000
Now, as a prerequisite, you need to understand that at least you should know Python programming language.

343
00:18:25,000 --> 00:18:30,000
You should know databases like MongoDB or PostgreSQL, at least how to write some kind of queries.

344
00:18:30,000 --> 00:18:32,000
Okay, that will be more than sufficient.

345
00:18:32,000 --> 00:18:38,000
Along with this, uh, some some some important, some some some basic understanding about git.

346
00:18:38,000 --> 00:18:39,000
Okay.

347
00:18:39,000 --> 00:18:43,000
But anyhow, what I have done is that I've given recordings of the entire Python.

348
00:18:43,000 --> 00:18:49,000
I've given the recordings of entire git, and then step by step we will follow each and every like how

349
00:18:49,000 --> 00:18:51,000
I've actually covered each and every modules over here.

350
00:18:51,000 --> 00:18:53,000
Similarly, we will go ahead and cover over there.

351
00:18:53,000 --> 00:18:57,000
One more I forgot to add as a logo that is nothing but DAGs hub.

352
00:18:57,000 --> 00:18:58,000
So it is a remote repository.

353
00:18:58,000 --> 00:19:04,000
And uh, we will also show you like if you are working in a collaborative environment, uh, how DAGs

354
00:19:04,000 --> 00:19:05,000
will be useful.

355
00:19:05,000 --> 00:19:10,000
So I hope you got an idea about the different, different things that we are going to cover in this

356
00:19:10,000 --> 00:19:11,000
course.

357
00:19:11,000 --> 00:19:13,000
Um, thank you for considering the course.

358
00:19:13,000 --> 00:19:15,000
I hope you enjoy this particular course.

359
00:19:15,000 --> 00:19:16,000
You learn a lot of things.

360
00:19:16,000 --> 00:19:21,000
Everything will be practical oriented, and we'll try to develop all our projects like this end to end.

361
00:19:21,000 --> 00:19:23,000
So yes, this was it from my side.

362
00:19:23,000 --> 00:19:24,000
I'll see you in the next video.

363
00:19:24,000 --> 00:19:26,000
So let's start the next module.

364
00:19:26,000 --> 00:19:27,000
That is with respect to Python.

365
00:19:27,000 --> 00:19:33,000
But before that we will go ahead and the installation of, you know, uh, the VSCode Anaconda and many

366
00:19:33,000 --> 00:19:34,000
more things.

367
00:19:34,000 --> 00:19:35,000
So yes, this was it from my side.

368
00:19:35,000 --> 00:19:36,000
I'll see you all in the next video.

